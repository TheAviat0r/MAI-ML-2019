{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 7. Градиентный бустинг\n",
    "\n",
    "Кратко вспомним материалы прошлой лекции.\n",
    "\n",
    "### Bias and Variance\n",
    "\n",
    "Модели машинного обучения подвержены ошибкам трех видов:\n",
    "\n",
    "* Неустранимая ошибка целевой переменной\n",
    "* Ошибка смещения (**Bias**) - \"недообучение\"\n",
    "* Ошибка разброса (**Variance**) - \"переобучение\"\n",
    "\n",
    "Искусство построения моделей - поиск оптимальной точки между смещением и разбросом (**Bias-Variance trade-off**)\n",
    "\n",
    "<img src=\"img/0_bias_variance_0.jpg\" height=80% width=80%>\n",
    "<img src=\"img/0_bias_variance_1.jpg\" height=80% width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ансамбли моделей\n",
    "\n",
    "* Объединение моделей в ансамбли позволяет повысить качество\n",
    "* Улучшение происходит за счет сглаживания индивидуальных предсказаний моделей\n",
    "* Если $p$ (вероятность правильного ответа) > 0.5, то у нас все должно сложиться хорошо\n",
    "* Чем разнообразнее модели в ансамбле, тем лучше\n",
    "\n",
    "Пусть на выборке $(X, y)$ обучены базовые модели - $b_1(x)$, $b_2(x)$, ... $b_m(X)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда (в задаче классификации) для \"учета мнения\" всех базовых алгоритмов используют:\n",
    "\n",
    "* Простое голосование большинством\n",
    "\n",
    "$$ f(x) = \\dfrac{1}{m}\\sum^{m}_{i=1}b_i(x) $$\n",
    "\n",
    "* Взвешенное голосование\n",
    "\n",
    "$$ f(x) = \\dfrac{1}{m}\\sum^{m}_{i=1}w_ib_i(x) $$\n",
    "\n",
    "Где $w_i$ - вес соответствующей модели $b_i$ в ансамбле\n",
    "\n",
    "Та же самая логика работает и в случае задачи регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging & Random Forest\n",
    "\n",
    "Было рассмотрено 3 подхода к построению ансамблей из независимых моделей:\n",
    "\n",
    "* Bagging\n",
    "* Метод случайных подпространств\n",
    "* Random Forest\n",
    "\n",
    "<img src=\"img/1_tree_ensembles.jpg\" height=80% width=80%>\n",
    "\n",
    "* В алгоритме Random Forest используются **решающие деревья**\n",
    "* Каждое дерево **независимо** обучается на случайной подвыборке объектов со случайно выбранными признаками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/1_tree_picture.jpg\" width=80% height=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/1_rf_prediction.jpg\" height=80% width=80%>\n",
    "\n",
    "* Почему решающее дерево - хороший кандидат для базовой модели в бэггинге/случайном лесе?\n",
    "* Зачем в случайном лесе брать подпространство признаков?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интуиция: глобально\n",
    "\n",
    "* Строили базовые модели независимо друг от друга\n",
    "* А что если вместо этого строить модели последовательно, на каждой итерации учитывая \"опыт прошлых поколений\"?\n",
    "* Назовем такой подход бустингом (**boosting**)\n",
    "\n",
    "Итоговый ансамбль:\n",
    "$$\\large a(x)=\\sum^{T}_{i=1}w_ib_i(x)$$\n",
    "\n",
    "Основное отличие от Random Forest - здесь модель $b_i(x)$ была построена на i-ой итерации алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интуиция: weights-based boosting\n",
    "\n",
    "Начнем с подхода под названием _**\"weights-based boosting\"**_ - бустинг, основанный на весах объектов.\n",
    "\n",
    "<img src=\"img/1_wbb_0.jpg\">\n",
    "\n",
    "$$\\large a(x)=b_1(x)$$\n",
    "\n",
    "<img src=\"img/1_wbb_1.jpg\">\n",
    "\n",
    "Обучили базовый алгоритм $b_1(x)$, применили его на обучающей выборке и получили предсказанные вероятности $\\large y_{pred,1}$\n",
    "\n",
    "$$\\large a(x)=b_1(x)$$\n",
    "\n",
    "<img src=\"img/1_wbb_2.jpg\">\n",
    "\n",
    "После чего перевзвесим объекты:\n",
    "\n",
    "$$\\large new\\_weight_i = \\dfrac{abs\\_error_i}{\\sum_jabs\\_error_j}$$\n",
    "\n",
    "<img src=\"img/1_wbb_3.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После первой итерации имеем:\n",
    "\n",
    "* Веса теперь не распределены равномерно\n",
    "* Модель $b_2$ будет уделять \"больше внимания\" объектам, которые после предсказания от $b_1$ получили больший вес.\n",
    "\n",
    "$$\\large a(x) = b_1(x) + b_2(x)$$\n",
    "\n",
    "Проделаем аналогичные шаги для обновленного веса:\n",
    "\n",
    "<img src=\"img/1_wbb_4.jpg\">\n",
    "<img src=\"img/1_wbb_5.jpg\">\n",
    "\n",
    "Будем снова перевзвешивать объекты, однако теперь мы будем использовать ошибку модели $\\large b_2(x)$\n",
    "<img src=\"img/1_wbb_6.jpg\">\n",
    "<img src=\"img/1_wbb_7.jpg\">\n",
    "\n",
    "Шаг 3. Повторяем описанную выше процедуру\n",
    "\n",
    "$$\\large a(x) = b_1(x) + b_2(x) + b_3(x) $$\n",
    "\n",
    "...\n",
    "\n",
    "### Интуитивные соображения:\n",
    "\n",
    "* Каждая новая модель будет пытаться \"исправить\" ошибки предыдущей\n",
    "* Используем _learning rate_ (\"темп обучения\"), чтобы не получалось так, что предсказания некоторых моделей не \"задавливали\" предсказания других\n",
    "\n",
    "$$\\large a(x) = \\eta b_1(x) + \\eta b_2(x) + \\eta b_3(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бустинг - задача бинарной классификации\n",
    "\n",
    "Обучающая выборка $(X, y)$ из $N$ объектов, $y \\in \\{-1, 1\\}$\n",
    "\n",
    "Модель взвешенного голосования:\n",
    "\n",
    "$$\\large a(x)=sign(\\sum^{T}_{i=1}w_i b_i(x)) $$\n",
    "\n",
    "Функция потерь:\n",
    "\n",
    "$$\\large loss = \\dfrac{1}{N}\\sum^{N}_{j=1}I(y_j \\neq a(x_j))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/2_losses.jpg\" height=80% width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Missclassification $= I(sign(f) \\neq y)$\n",
    "* Exponential $= exp(-yf)$\n",
    "\n",
    "Оптимизация _exponential loss_ приводит нас к алгоритму **AdaBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "**Алгоритм:**\n",
    "\n",
    "* Задаем начальное распределение весов объектов в обучающей выборке:\n",
    "    $$\\large\\alpha_j^0 = \\dfrac{1}{N}, j=1,2,\\dots,N$$\n",
    "\n",
    "* Для каждого ***i*** от ***1*** до ***T***:\n",
    "  * Строим классификатор $b_i$ с использованием весов $\\{w^{i-1}\\}$\n",
    "  * Вычисляем ошибку $b_i$: $$\\large err_i = \\sum^{N}_{j=1}\\alpha^{i-1}_j I[y_j \\neq b_i(x_j)]$$\n",
    "  * Вычисляем весовой коэффициент для $b_i$: $$\\large w_i = \\dfrac{1}{2}ln\\dfrac{1 - err_i}{err_i}$$\n",
    "  * Обновляем веса объектов: $$\\large \\alpha_j^i = \\alpha_j^{i-1}e^{w_iI[y_j \\neq b_i(x_j)]}$$\n",
    "  * Нормируем веса обучающих объектов: $$\\large \\alpha_j^i = \\dfrac{\\alpha_j^i}{\\sum_{k=1}^{N}\\alpha_k^i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример на пеньках\n",
    "\n",
    "* Решаем задачу классификации\n",
    "* Базовые алгоритмы - \"пни\" (решающие деревья глубины 1)\n",
    "* 10 объектов в выборке\n",
    "* Их начальные веса - 0.1\n",
    "\n",
    "<img src=\"img/4_pen_0.jpg\" height=30% width=30%>\n",
    "\n",
    "* Строим пень #1\n",
    "* Он ошибается на 3 объектах\n",
    "* Его вес - 0.42\n",
    "\n",
    "<img src=\"img/4_pen_1.jpg\" height=30% width=30%>\n",
    "\n",
    "* Увеличиваем вес объектов с ошибкой\n",
    "\n",
    "<img src=\"img/4_pen_2.jpg\" height=30% width=30%>\n",
    "\n",
    "* Строим пень #2\n",
    "* Также ошибается на 3 объектах\n",
    "* Однако его вес уже 0.65, т.к. он лучше определяет более \"тяжелые\" объекты\n",
    "\n",
    "<img src=\"img/4_pen_5.jpg\" height=30% width=30%>\n",
    "\n",
    "* Увеличиваем вес объектов с ошибкой\n",
    "\n",
    "<img src=\"img/4_pen_6.jpg\" height=30% width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Строим пень #3\n",
    "* Снова ошибается на 3 объектах\n",
    "* Вес этого пня - 0.92\n",
    "\n",
    "<img src=\"img/4_pen_7.jpg\" height=30% width=30%>\n",
    "\n",
    "Итоговый алгоритм - взвешенная композиция базовых алгоритмов\n",
    "\n",
    "<img src=\"img/4_pen_9.jpg\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Экспериментальный факт\n",
    "\n",
    "<img src=\"img/5_fact.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual-based boosting\n",
    "\n",
    "_Residual-based boosting_ = бустинг, основанный на остатках\n",
    "\n",
    "$$\\large a(x) = b_1(x)$$\n",
    "\n",
    "<img src=\"img/6_resid_0.jpg\">\n",
    "\n",
    "* Строим модель $b_1(x)$\n",
    "* Делаем предсказания при помощи этой модели\n",
    "\n",
    "<img src=\"img/6_resid_1.jpg\">\n",
    "\n",
    "$$\\large a(x) = b_1(x)$$\n",
    "\n",
    "<img src=\"img/6_resid_2.jpg\">\n",
    "\n",
    "Для каждого объекта из обучающей выборки считаем невязку (**residual**) модели $b_1(x)$: $y - y_{pred}$\n",
    "\n",
    "<img src=\"img/6_resid_3.jpg\">\n",
    "\n",
    "Делаем полученную невязку таргетом для обучения следующей модели - $b_2(x)$\n",
    "\n",
    "$$\\large a(x) = b_1(x)$$\n",
    "\n",
    "<img src=\"img/6_resid_4.jpg\">\n",
    "\n",
    "$$\\large a(x) = b_1(x) + b_2(x)$$\n",
    "\n",
    "<img src=\"img/6_resid_5.jpg\">\n",
    "\n",
    "Снова считаем невязку\n",
    "\n",
    "<img src=\"img/6_resid_7.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large a(x) = b_1(x) + b_2(x)$$\n",
    "\n",
    "<img src=\"img/6_resid_8.jpg\">\n",
    "\n",
    "И делаем таргетом для модели $b_3(x)$\n",
    "\n",
    "Проделываем аналогичные шаги для $b_3(x)$, $b_4(x)$, $b_5(x)$, ..., $b_T(x)$\n",
    "\n",
    "Итоговый ансамбль выглядит так:\n",
    "\n",
    "$$\\large a(x) = b_1(x) + b_2(x) + b_3(x) + \\dots + b_T(x) = \\sum^T_{i=1}b_i(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "* Модель: $$\\large a(x) = \\sum^{T}_{i=1}b_i(x)$$\n",
    "\n",
    "* Ошибка композиции на train выборке:\n",
    "\n",
    "$$\\large err(a) = \\sum^{N}_{j=1}L(y_j, a(x_j) = \\sum^{N}_{j=1}L(y_j, \\sum^{T}_{i=1}w_ib_i(x)) = $$\n",
    "$$\\large = \\sum^{N}_{j=1}L(y_j, \\sum^{T-1}_{i=1}w_ib_i(x) + wb(x_j) \\rightarrow \\underset{w,b}{min}$$\n",
    "\n",
    "* На i-ой итерации при построении модели все предыдущие зафиксированы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting: алгоритм\n",
    "\n",
    "* Инициализируем ансамбль:\n",
    "\n",
    "$$\\large a_0(x) = \\underset{C}{argmin}\\sum^{N}_{i=1}L(y_i, C)$$\n",
    "\n",
    "* Для всех i от 1 до **T**:\n",
    "  * Для всех $j$ от $1$ до $N$ вычисляем градиент функции потерь по ответам ансамбля $\\large g_{ij}$:\n",
    "    $$\\large g_{ij} = -[\\dfrac{\\partial L(y_i, a(x_j)}{\\partial a(x_j)}]_{a=a_{i-1}}$$\n",
    "    \n",
    "  * Строим $\\large b_i$ с целевой переменной $\\large g_{ij}$:\n",
    "    $$\\large b_i = \\underset{b}{argmin}\\space(\\sum^N_{j=1}(g_{ij} - b(x_j))^2$$\n",
    "    \n",
    "  * Вес $w_i$ модели $b_i$:\n",
    "    $$\\large w_i = \\underset{w}{argmin}\\space\\sum^N_{j=1}L(y_j, a_{i-1}(x_j) + wb_i(x_j))$$\n",
    "    \n",
    "  * Добавляем в ансабмль:\n",
    "    $$\\large a_i(x) = a_{i-1}(x) + w_ib_i(x)$$\n",
    "* Ансамбль на выходе: $$\\large a(x) = a_T(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting: задача регрессии\n",
    "\n",
    "* Задача регрессии, функция потерь - MSE\n",
    "* Модель - **a(x)**\n",
    "* Функция потерь на одном объекте $(x_j, y_j)$:\n",
    "\n",
    "$$\\large L_j = L(y_j, a(x_j) = MSE(y_j, a(x_j)) = \\dfrac{1}{2}(y_j - a(x_j))^2$$\n",
    "\n",
    "* Анти-градиент функции потерь по модели **a**:\n",
    "\n",
    "$$\\large -\\dfrac{\\partial L_j}{\\partial a} = - \\dfrac{1}{2} \\dfrac{\\partial(y_j - a(x_j))^2}{\\partial a} = \n",
    "  y_j - a(x_j)$$\n",
    "  \n",
    "* MSE на одном объекте - разница между реальным значением и таргетом (как в интуитивном примере выше)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting: learning rate\n",
    "\n",
    "* Полученные на i-ой итерации модели с какой-то точностью аппроксимируют градиент функции потерь\n",
    "* Не стоит полностью доверять данной аппроксимации\n",
    "* Поэтому добавим постоянный коэффициент $\\eta$ перед моделями - **learning_rate**:\n",
    "\n",
    "$$\\large a_i(x) = a_{i-1}(x) + \\eta w_ib_i(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting\n",
    "\n",
    "* Использование bagging'a при построении деревьев может улучшить обощающую способность\n",
    "* Больше вероятность попадания в локальный минимум функции потерь\n",
    "* Можно использовать OOB error\n",
    "\n",
    "<img src=\"img/5_shrinkage.jpg\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "* Библиотека для обучения моделей градиентного бустинга\n",
    "* ~ Extreme Gradient Boosting\n",
    "* Особое внимание уделено регуляризации деревьев\n",
    "* Разрабатывается с 2014 года\n",
    "\n",
    "https://github.com/dmlc/xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost: регуляризация деревьев\n",
    "\n",
    "Основная идея - модификация критерия разбиения\n",
    "\n",
    "$$\\large Impurity_{node} = -\\dfrac{1}{2}\\dfrac{G_j^2}{H_j + \\lambda} + \\gamma T$$\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/tutorials/model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "\n",
    "Библиотека для градиентного бустинга от Microsoft\n",
    "\n",
    "Быстрее XGBoost\n",
    "\n",
    "Деревья строятся в глубину (**Leaf-wise tree growth**)\n",
    "\n",
    "https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обзор настроек\n",
    "\n",
    "* Общие параметры:\n",
    "  * Тип задачи (objective)\n",
    "  * Функция потерь (loss) - оптимизируемая величина\n",
    "  * Метрика качества (eval_metric) - контрольная величина\n",
    "* Параметры композиции\n",
    "  * Темп обучения (learning_rate)\n",
    "  * Число итераций (n_estimators)\n",
    "* Параметры одного дерева\n",
    "  * Максимальная глубина (max_depth)\n",
    "*\n",
    "...\n",
    "* Параметры сэмплирования\n",
    "  * Доля объектов / признаков для построения дерева\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итоги:\n",
    "\n",
    "* На больших и сложных объемах данных - один из лучших алгоритмов\n",
    "* Много гиперпараметров\n",
    "* Обычно строится на деревьях решений\n",
    "* Есть быстрые реализации"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
